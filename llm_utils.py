"""
llm_utils.py
------------
Utilities for combining files using an LLM (OpenAI API) for the Confluence Downloader project.
"""
import openai
import os
from tqdm import tqdm
import time
from itertools import cycle
import threading
import requests
import logging

def combine_files_with_llm(file_paths, output_dir, api_key, model="gpt-4", output_filename="LLM_Combined.md", overwrite_mode="overwrite", free_prompt_mode="default"):
    """
    Combine the given files using an LLM and save the result as a Markdown file.

    Args:
        file_paths (list): List of file paths to combine.
        output_dir (str): Directory to save the combined file.
        api_key (str): OpenAI API key.
        model (str): OpenAI model to use (default: gpt-4).
        output_filename (str): Name of the output Markdown file.
        overwrite_mode (str): 'overwrite' (default) or 'increment' to avoid overwriting existing files.
        free_prompt_mode (str): For free models, choose which prompt to use: 'default' (detailed) or 'quick' (short, direct). Default is 'default'.
    Returns:
        str: Path to the combined Markdown file, or None on error.
    """
    # Paid prompt: for paid models (e.g., gpt-4)
    paid_prompt = (
        "You are acting as a professional technical writer. Your goal is to create a single, comprehensive, and easy-to-navigate Markdown document that combines the content of all these files. "
        "The final document should be accurate, logically organized, visually clear, and suitable for onboarding, reference, and knowledge transfer for both new and experienced users.\n"
        "Combine the content of all these files into a well-structured Markdown document.\n"
        "At the very top of the document, add a short note explaining that all content marked with a  emoji or special formatting was generated by an AI assistant and is not present in the original files.\n"
        "Immediately after, add a 'Formatting Guide' section that describes:\n"
        "  - How LLM-supplied content is marked ( for paragraphs, inline code or italics for small contributions).\n"
        "  - That original content is unmarked.\n"
        "  - That the document uses Markdown formatting (headings, lists, tables, blockquotes, etc.) for clarity and navigation, and readers are encouraged to take advantage of these features.\n"
        "- Ensure that all unique information from each file is preserved and not lost.\n"
        "- Remove any duplicate or redundant content.\n"
        "- Organize the document into clear, logical sections with appropriate headings and subheadings.\n"
        "- If the document is long, create a table of contents at the top.\n"
        "- Group related information together, and reorder content as needed to improve logical flow and coherence.\n"
        "- Where possible, cross-reference related sections and synthesize information from multiple sources to provide a unified explanation.\n"
        "- Resolve any inconsistencies between files, and note any ambiguities or gaps in the source material.\n"
        "- Enhance readability by rewriting awkward or unclear passages, correcting grammar, and improving transitions between sections.\n"
        "- Where helpful, add actionable summaries, best practices, or checklists for users.\n"
        "- Use bullet points, tables, and diagrams (in Markdown) to clarify complex information.\n"
        "- Where possible, add summaries or introductions to major sections for clarity.\n"
        "- Where helpful, add additional context such as: summaries of tools, version numbers, summaries of broad topics, and lists of important keywords.\n"
        "- Call out important warnings, tips, or caveats using blockquotes or bold formatting.\n"
        "- Mark all supplemental context you provide (not found in the original files):\n"
        "    - For descriptions or paragraphs, prefix with a robot emoji .\n"
        "    - For smaller contributions (e.g., a single word, keyword, or short phrase), use small color formatting such as inline code or italics.\n"
        "- Do not omit any important technical or contextual details from the original files.\n"
        "- The final document should be easy to navigate, comprehensive, and professional in quality."
    )
    # Reduced prompt: for free models (e.g., gpt-3.5-turbo)
    reduced_prompt = (
        "You are a technical writer. Combine the content of all these files into a single, well-structured Markdown document.\n"
        "- Organize the document with clear headings and sections.\n"
        "- Remove duplicate content.\n"
        "- Keep all unique information.\n"
        "- Make the document easy to read and navigate.\n"
        "- Add a short note at the top that content marked with  was generated by AI.\n"
        "- Use Markdown formatting."
    )
    # Quick free prompt: for free models (e.g., gpt-3.5-turbo)
    quick_free_prompt = (
        "combine these files into 1. preserve all unique information. "
        "improve readability and flow. create sections and reorder information based on need and where applicable"
    )
    # Choose prompt based on model and free_prompt_mode
    paid_models = ["gpt-4", "gpt-4-turbo", "gpt-4-32k", "gpt-4o"]
    if any(m in model for m in paid_models):
        prompt = paid_prompt
    else:
        if free_prompt_mode == "quick":
            prompt = quick_free_prompt
        else:
            prompt = reduced_prompt
    print("\n[LLM] Preparing to read files for combination...")
    combined_content = ""
    # Read and concatenate all file contents, separating with headers
    for file_path in tqdm(file_paths, desc="Reading files for LLM combine"):
        try:
            with open(file_path, "r", encoding="utf-8") as f:
                file_content = f.read()
            combined_content += f"\n\n---\n\n# {os.path.basename(file_path)}\n\n" + file_content
        except Exception as e:
            print(f"Error reading {file_path}: {e}")
    print("[LLM] Finished reading and preparing files.")
    # Set the OpenAI API key
    openai.api_key = api_key
    try:
        stop_spinner = False
        max_retries = 3
        retry_delay = 5  # seconds, will be doubled on each retry
        attempt = 0
        while attempt < max_retries:
            spinner_thread = None
            try:
                print("[LLM] Sending content to OpenAI for combination. This may take a few moments...")
                def spinner():
                    for c in cycle(['|', '/', '-', '\\']):
                        if stop_spinner:
                            break
                        print(f'\r[LLM] Waiting for OpenAI response... {c}', end='', flush=True)
                        time.sleep(0.1)
                    print('\r', end='', flush=True)
                spinner_thread = threading.Thread(target=spinner)
                spinner_thread.start()
                # Call the OpenAI chat completion API with timeout
                response = openai.chat.completions.create(
                    model=model,
                    messages=[
                        {"role": "system", "content": "You are a helpful technical writer."},
                        {"role": "user", "content": prompt + "\n\n" + combined_content}
                    ],
                    max_tokens=4096,  # adjust as needed
                    timeout=60  # seconds
                )
                llm_output = response.choices[0].message.content
                print("[LLM] Received response from OpenAI.")
                break  # Success, exit retry loop
            except Exception as e:
                # Try to extract HTTP status code and response body if available
                err_msg = str(e)
                status_code = None
                response_body = None
                if hasattr(e, 'status_code'):
                    status_code = e.status_code
                if hasattr(e, 'response') and e.response is not None:
                    try:
                        response_body = e.response.text
                    except Exception:
                        response_body = None
                print(f"Error calling OpenAI API (attempt {attempt+1}/{max_retries}): {err_msg}")
                if status_code:
                    print(f"[LLM] HTTP status code: {status_code}")
                if response_body:
                    print(f"[LLM] Response body: {response_body}")
                logging.error(f"[LLM] OpenAI API error (attempt {attempt+1}/{max_retries}): {err_msg}")
                if status_code:
                    logging.error(f"[LLM] HTTP status code: {status_code}")
                if response_body:
                    logging.error(f"[LLM] Response body: {response_body}")
                # Retry on 400, 429, 500 errors, otherwise fail immediately
                if (status_code and status_code in [400, 429, 500]) or 'Bad Request' in err_msg or '429' in err_msg or '500' in err_msg:
                    attempt += 1
                    if attempt < max_retries:
                        print(f"[LLM] Retrying in {retry_delay} seconds...")
                        logging.info(f"[LLM] Retrying OpenAI API call in {retry_delay} seconds (attempt {attempt+1}/{max_retries})...")
                        time.sleep(retry_delay)
                        retry_delay *= 2
                    else:
                        print(f"[LLM] Max retries reached. Failing.")
                        logging.error(f"[LLM] Max retries reached for OpenAI API call.")
                        return None
                else:
                    return None
            finally:
                stop_spinner = True
                if spinner_thread is not None:
                    spinner_thread.join()
    except Exception as e:
        print(f"Error calling OpenAI API: {e}")
        return None
    # Write the LLM's output to the specified Markdown file
    output_path = os.path.join(output_dir, output_filename)
    if overwrite_mode == "increment":
        name, ext = os.path.splitext(output_filename)
        i = 2
        conflict_detected = False
        while os.path.exists(output_path):
            if not conflict_detected:
                logging.info(f"[LLM Combine] Filename conflict detected for {output_path}, triggering increment logic.")
                conflict_detected = True
            new_filename = f"{name}_{i}{ext}"
            output_path = os.path.join(output_dir, new_filename)
            i += 1
        if conflict_detected:
            logging.info(f"[LLM Combine] Final incremented output filename: {output_path}")
    else:
        if os.path.exists(output_path):
            logging.info(f"[LLM Combine] Overwriting existing file: {output_path}")
        else:
            logging.info(f"[LLM Combine] No conflict, writing to: {output_path}")
    try:
        print(f"[LLM] Writing combined content to {output_path} ...")
        for _ in tqdm(range(20), desc="Writing file", ncols=70):
            time.sleep(0.01)  # Simulate progress for user feedback
        with open(output_path, "w", encoding="utf-8") as f:
            f.write(llm_output)
        print(f"[LLM] Successfully wrote combined file: {output_path}")
    except Exception as e:
        print(f"Error writing LLM-combined file: {e}")
        return None
    return output_path 